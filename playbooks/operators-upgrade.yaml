- hosts: master
  vars_files:
    - cns_values.yaml
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
  tasks:
   - name: Validate whether Kubernetes cluster installed
     shell: kubectl cluster-info
     register: k8sup
     no_log: True
     failed_when: false

   - name: Reload the CRI-O service
     when: container_runtime == 'cri-o'
     become: true
     systemd:
       state: restarted
       name: "{{ item }}"
     with_items:
       - crio
       - cri-o

   - name: Remove Values file
     file:
       path: "{{ ansible_user_dir }}/values.yaml"
       state: absent

   - name: Check Current Running Cloud Native Stack Version
     shell: kubectl version -o json | jq .serverVersion.gitVersion | sed 's/\"//g'
     register: k8sversion

   - name: Uninstall the GPU Operator with MIG
     shell: |
       kubectl label nodes --all nvidia.com/mig.config=all-disabled --overwrite
       sleep 5
       config_state=$(kubectl describe nodes  |grep mig.config.state |head -n1 | awk -F'=' '{print $2}')
       while [ $config_state != "success" ]
       do
         sleep 5
         config_state=$(kubectl describe nodes  |grep mig.config.state | head -n1 |awk -F'=' '{print $2}')
       done
     when: "enable_mig == true and cns_version >= 4.1"
     async: 120
     args:
       executable: /bin/bash

   - name: Add custom Helm repo
     shell: " {{ item }}"
     with_items:
        - helm repo add nvidia '{{ helm_repository }}' --force-update --username=\$oauthtoken --password='{{ ngc_registry_password }}'
        - helm repo update
     when: 'ngc_registry_password != ""'

   - name: Upgrade the Network Operator values on Cloud Native Stack
     when: "enable_network_operator == true"
     shell: "{{ item }}"
     with_items:
       - helm pull mellanox/network-operator --version {{ network_operator_version }} --untar --untardir network-operator-chart
       - kubectl apply -f network-operator-chart/network-operator/crds -f network-operator-chart/network-operator/charts/sriov-network-operator/crds
       - rm -rf network-operator-chart
       - kubectl scale deployment --replicas=0 -n network-operator network-operator
       - sleep 20

   - name: Upgrade Network Operator on Cloud Native Stack
     when: "enable_network_operator == true"
     shell: "helm upgrade $(helm ls -A | grep network-operator | awk '{print $1}') mellanox/network-operator --version {{ network_operator_version }} --namespace network-operator --values {{ ansible_user_dir }}/network-operator-values.yaml"

   - name: Upgrade GPU Operator CRDs on Cloud Native Stack
     when: "enable_gpu_operator == true and k8sversion.stdout == 'v1.29.4' or enable_gpu_operator == true and k8sversion.stdout == 'v1.28.8' or enable_gpu_operator == true and k8sversion.stdout == 'v1.27.12'"
     shell: "{{ item }}"
     with_items:
       - kubectl delete crd clusterpolicies.nvidia.com nvidiadrivers.nvidia.com
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.3.0/deployments/gpu-operator/crds/nvidia.com_clusterpolicies_crd.yaml
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.3.0/deployments/gpu-operator/charts/node-feature-discovery/crds/nfd-api-crds.yaml
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.3.0/deployments/gpu-operator/crds/nvidia.com_nvidiadrivers.yaml
       - helm show --version=v{{ gpu_operator_version }} values nvidia/gpu-operator > {{ ansible_user_dir }}/values.yaml

   - name: Upgrade GPU Operator CRDs on Cloud Native Stack
     when: "enable_gpu_operator == true and k8sversion.stdout == 'v1.29.6' or enable_gpu_operator == true and k8sversion.stdout == 'v1.28.12' or enable_gpu_operator == true and k8sversion.stdout == 'v1.30.2'"
     shell: "{{ item }}"
     with_items:
       - kubectl delete crd clusterpolicies.nvidia.com nvidiadrivers.nvidia.com
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.6.2/deployments/gpu-operator/crds/nvidia.com_clusterpolicies_crd.yaml
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.6.2/deployments/gpu-operator/charts/node-feature-discovery/crds/nfd-api-crds.yaml
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.6.2/deployments/gpu-operator/crds/nvidia.com_nvidiadrivers.yaml
       - helm show --version=v{{ gpu_operator_version }} values nvidia/gpu-operator > {{ ansible_user_dir }}/values.yaml

   - name: Upgrade GPU Operator CRDs on Cloud Native Stack
     when: "enable_gpu_operator == true and k8sversion.stdout == 'v1.29.9' or enable_gpu_operator == true and k8sversion.stdout == 'v1.30.5'"
     shell: "{{ item }}"
     with_items:
       - kubectl delete crd clusterpolicies.nvidia.com nvidiadrivers.nvidia.com
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.9.2/deployments/gpu-operator/crds/nvidia.com_clusterpolicies.yaml
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.9.2/deployments/gpu-operator/charts/node-feature-discovery/crds/nfd-api-crds.yaml
       - kubectl apply -f https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/v24.9.2/deployments/gpu-operator/crds/nvidia.com_nvidiadrivers.yaml
       - helm show --version=v{{ gpu_operator_version }} values nvidia/gpu-operator > {{ ansible_user_dir }}/values.yaml

   - name: create GPU Custom Values for proxy
     when: proxy == true
     replace:
       dest: "{{ ansible_user_dir }}/values.yaml"
       regexp: '  env: \[\]'
       replace: "  env:\n    - name: HTTPS_PROXY\n      value: {{ https_proxy }}\n    - name: HTTP_PROXY\n      value: {{ http_proxy }}\n    - name: https_proxy\n      value: {{ https_proxy }}\n    - name: http_proxy\n      value: {{ http_proxy }}\n    - name: NO_PROXY\n      value: {{ network.stdout }},localhost,127.0.0.0/8,10.96.0.1/24,10.244.0.0/16,192.168.32.0/22,{{ subnet.stdout }}.0/24\n    - name: no_proxy\n      value: {{ network.stdout }},localhost,127.0.0.0/8,10.96.0.1/24,10.244.0.0/16,192.168.32.0/22,{{ subnet.stdout }}.0/24"

   - name: Upgrade GPU Operator with Pre Installed NVIDIA Driver on Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == true and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false"
     shell: "helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --namespace nvidia-gpu-operator --values {{ ansible_user_dir }}/values.yaml --set driver.enabled=false,toolkit.enabled=false "

   - name: Installing the GPU Operator with Pre Installed NVIDIA Driver and MIG on NVIDIA Cloud Native Stack
     when: "cns_docker == true and enable_mig == true and enable_rdma == false  and enable_vgpu == false and gpu_operator.rc >= 1 and ngc_registry_password == ''"
     shell: helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --namespace nvidia-gpu-operator --set mig.strategy=single,driver.enabled=false,toolkit.enabled=false --wait --generate-name

   - name: Upgrade GPU Operator on Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false and ngc_registry_password == ''"
     shell: "helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --namespace nvidia-gpu-operator --values {{ ansible_user_dir }}/values.yaml --set driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}'"

   - name: Upgrading the Signed GPU Operator on NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == true and ngc_registry_password == ''"
     shell: "{{ item }}"
     with_items:
       - helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --set driver.version={{ driver_version }},driver.usePrecompiled=true,driver.repository={{ gpu_operator_driver_registry }} --wait --generate-name
       - sleep 20
       - kubectl patch clusterpolicy/cluster-policy --type='json' -p='[{"op":"replace", "path":"/spec/driver/usePrecompiled", "value":true},{"op":"replace", "path":"/spec/driver/version", "value":"{{ driver_version }}"}]'

   - name: Upgrading the GPU Operator with vGPU on NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == false and enable_vgpu == true and enable_rdma == false and enable_gds == false and ngc_registry_password == ''"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --namespace nvidia-gpu-operator --values {{ ansible_user_dir }}/values.yaml --set driver.repository='{{ gpu_operator_driver_registry }}',driver.version='{{ gpu_driver_version }}',driver.imagePullSecrets[0]=registry-secret,driver.licensingConfig.configMapName=licensing-config

   - name: Upgrading the GPU Operator with MIG on NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == true and  enable_rdma == false and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and ngc_registry_password == ''"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --namespace nvidia-gpu-operator --values {{ ansible_user_dir }}/values.yaml --set mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == true and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and ngc_registry_password == ''"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --namespace nvidia-gpu-operator --values {{ ansible_user_dir }}/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}'

   - name: Upgrading the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and ngc_registry_password == ''"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --namespace nvidia-gpu-operator --values {{ ansible_user_dir }}/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}'

   - name: Upgrading the GPU Operator with RDMA and GDS with Host MOFEDon NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false and ngc_registry_password == ''"
     shell:  helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator --version {{ gpu_operator_version }} --namespace nvidia-gpu-operator --values {{ ansible_user_dir }}/values.yaml --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,gds.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}'

   - name: Upgrade the GPU Operator with Network Operator on NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and 'running' in k8sup.stdout and cns_docker == false and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and ngc_registry_password == ''"
     shell: helm upgrade $(helm ls -A | grep gpu-oper | awk '{print $1}') nvidia/gpu-operator  --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --namespace nvidia-gpu-operator  --set driver.rdma.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}'
     
   - name: Enable MIG profile with GPU Operator on NVIDIA Cloud Native Stack
     when: "enable_gpu_operator == true and enable_mig == true and enable_vgpu == false and cns_version >= 4.1"
     shell: "kubectl label nodes --all nvidia.com/mig.config={{ mig_profile }} --overwrite"