- hosts: master
  vars_files:
    - cns_values.yaml
  tasks:
   - name: Validate whether Kubernetes cluster installed
     shell: kubectl cluster-info
     register: k8sup
     no_log: True
     failed_when: false

   - name: Checking Nouveau is disabled
     become: true
     command: lsmod | grep nouveau
     register: nouveau_result
     failed_when: false

   - name: Alert
     when: nouveau_result.rc != 1
     failed_when: false
     debug:
       msg: "Please reboot the host and run the same command again"

   - name: check DNS access
     shell: |
       kubectl delete pod dnsutils > /dev/null
       kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml > /dev/null
       status=$(kubectl get pods dnsutils | awk '{print $3}' | grep -v STATUS)
       while [ $status != 'Running' ]
       do
         sleep 10
         status=$(kubectl get pods dnsutils | awk '{print $3}' | grep -v STATUS)
       done
       kubectl exec -i -t dnsutils -- nslookup archive.ubuntu.com | grep 'Name:' | uniq | awk '{print $NF}' | head -n1
     register: dns_check
     retries: 3
     until: dns_check.stdout == 'archive.ubuntu.com'
     failed_when: false

   - name: DNS Alert
     debug:
       msg: "Looks like your system network doesn't reachable to archive.ubuntu.com, please update your DNS settings to proceed further"
     when: "dns_check.stdout != 'archive.ubuntu.com'"
     failed_when: true

   - name: Check NVIDIA Driver Modules are loaded
     shell: "lsmod | grep -i nvidia"
     register: nvidia_mod
     no_log: True
     ignore_errors: true
     failed_when: false

   - name: Check NVIDIA SMI loaded
     shell: "nvidia-smi"
     register: nvidia_smi
     no_log: True
     ignore_errors: true
     failed_when: false

   - name: NVIDIA Driver Alert
     debug:
       msg: "NVIIDA Driver is installed on the system, please uninstall the NVIDIA Driver and continue the installation. Example: 'sudo /usr/bin/nvidia-uninstall'"
     when: cns_nvidia_driver != true and nvidia_mod.rc > 1 or cns_nvidia_driver != true and nvidia_smi.rc == 0
     ignore_errors: true
     failed_when: false

   - name: GPU PCI ID
     shell: lspci | grep -i nvidia | awk '{print $1}' | head -n1
     failed_when: false
     register: pci

   - name: Reload the CRI-O service
     when: container_runtime == 'cri-o'
     become: true
     systemd:
       state: restarted
       name: "{{ item }}"
     with_items:
       - crio
       - cri-o

   - name: Label the Kubernetes nodes as worker
     failed_when: false
     command: 'kubectl label nodes --all node-role.kubernetes.io/worker='

   - name: Checking if Network Operator is installed
     shell: helm ls -A | grep network-operator
     register: network_operator
     failed_when: false
     no_log: True

   - name: Get the Active Mellanox NIC on master
     when: "enable_network_operator == true"
     become: true
     failed_when: false
     shell: "touch /tmp/$(hostname)-nic; for device in `lshw -class network -short | grep -i ConnectX | awk '{print $2}' | egrep -v 'Device|path' | sed '/^$/d'`;do echo -n $device; ethtool $device | grep -i 'Link detected'; done | awk '{print $1}' | grep 'en' > /tmp/$(hostname)-nic"

   - name: List Mellanox Active NICs
     when: "enable_network_operator == true"
     failed_when: false
     shell: "for list in `ls -lrt /tmp/*nic | awk '{print $NF}'`; do cat $list | tr '\n' ','; done | sed 's/.$//'"
     register: active_nic

   - name: Copy files to master
     when: "enable_network_operator == true"
     no_log: true
     copy:
       src: "{{ item }}"
       dest: "{{ ansible_user_dir }}/"
     with_fileglob:
       - "{{lookup('pipe', 'pwd')}}/files/network-operator-values.yaml"
       - "{{lookup('pipe', 'pwd')}}/files/network-operator-value.yaml"
       - "{{lookup('pipe', 'pwd')}}/files/networkdefinition.yaml"
       - "{{lookup('pipe', 'pwd')}}/files/mellanox-test.yaml"

   - name: Update Active mellanox NIC in network-operator-values.yaml
     when: "enable_network_operator == true and network_operator_version < '24.10.1'"
     failed_when: false
     shell: 'sed -i "s/devices: \[.*\]/devices: \\[ {{ active_nic.stdout }}\]/g" {{ ansible_user_dir }}/network-operator-values.yaml'

   - name: Generate NIC Cluster Policy template
     when: "enable_network_operator == true and network_operator_version >= '24.10.1'"
     copy:
       src: "{{lookup('pipe', 'pwd')}}/files/nic-cluster-policy.yaml"
       dest: "{{ ansible_user_dir }}/nic-cluster-policy.yaml.j2"

   - name: Create final NIC Cluster Policy configuration
     when: "enable_network_operator == true and network_operator_version >= '24.10.1'"
     ansible.builtin.template:
       src: "{{ ansible_user_dir }}/nic-cluster-policy.yaml.j2"
       dest: "{{ ansible_user_dir }}/nic-cluster-policy.yaml"

   - name: Update Active mellanox NIC in network-operator-values.yaml
     when: "enable_network_operator == true and cns_version >= 4.1"
     failed_when: false
     shell: 'sed -i "s/              \"ifNames\": \[.*\]/              \"ifNames\": \\[\"{{ active_nic.stdout }}\"\]/g" {{ ansible_user_dir }}/nic-cluster-policy.yaml'

   - name: Installing the Network Operator on NVIDIA Cloud Native Stack
     when: "enable_network_operator == true and cns_version >= 4.1"
     shell: "{{ item }}"
     with_items:
        - helm repo add mellanox '{{ helm_repository }}' --force-update
        - helm repo update
        - kubectl label nodes --all node-role.kubernetes.io/master- --overwrite

   - name: Installing the Network Operator on NVIDIA Cloud Native Stack
     when: "enable_network_operator == true and ansible_architecture == 'x86_64' and  network_operator_version < '24.10.1'"
     shell: "helm install --version {{ network_operator_version }} -f {{ ansible_user_dir }}/network-operator-values.yaml -n network-operator --create-namespace --wait network-operator mellanox/network-operator"

   - name: Installing the Network Operator on NVIDIA Cloud Native Stack
     when: "enable_network_operator == true and ansible_architecture == 'x86_64' and  network_operator_version >= '24.10.1'"
     shell: "helm install --version {{ network_operator_version }} -f {{ ansible_user_dir }}/network-operator-value.yaml -n network-operator --create-namespace --wait network-operator mellanox/network-operator"

   - name: Applying the NIC Cluster Policy on NVIDIA Cloud Native Stack
     when: "enable_network_operator == true and ansible_architecture == 'x86_64' and network_operator_version >= '24.10.1'"
     shell: "kubectl apply -f {{ ansible_user_dir }}/nic-cluster-policy.yaml"

   - name: Checking if GPU Operator is installed
     shell: helm ls -A | grep gpu-operator
     register: gpu_operator
     failed_when: false
     no_log: True

   - name: Checking if Network Operator is installed
     shell: helm ls -A | grep network-operator
     register: network_operator_valid
     failed_when: false
     no_log: True

   - name: Add nvidia Helm repo
     shell: " {{ item }}"
     with_items:
        - helm repo add nvidia '{{ helm_repository }}' --force-update
        - helm repo update
     when: 'ngc_registry_password == ""'

   - name: Add custom Helm repo
     shell: " {{ item }}"
     with_items:
        - helm repo add nvidia '{{ helm_repository }}' --force-update --username=\$oauthtoken --password='{{ ngc_registry_password }}'
        - helm repo update
     when: 'ngc_registry_password != ""'

   - name: Get the GPU Operator Values.yaml
     shell: helm show --version=v{{ gpu_operator_version }} values '{{ gpu_operator_helm_chart }}' > {{ ansible_user_dir }}/values.yaml
     when: "enable_gpu_operator == true"

   - name: create GPU Custom Values for proxy
     when: proxy == true
     replace:
       dest: "{{ ansible_user_dir }}/values.yaml"
       regexp: '  env: \[\]'
       replace: "  env:\n    - name: HTTPS_PROXY\n      value: {{ https_proxy }}\n    - name: HTTP_PROXY\n      value: {{ http_proxy }}\n    - name: https_proxy\n      value: {{ https_proxy }}\n    - name: http_proxy\n      value: {{ http_proxy }}\n    - name: NO_PROXY\n      value: {{ network.stdout }},localhost,127.0.0.0/8,10.96.0.1/24,10.244.0.0/16,192.168.32.0/22,{{ subnet.stdout }}.0/24\n    - name: no_proxy\n      value: {{ network.stdout }},localhost,127.0.0.0/8,10.96.0.1/24,10.244.0.0/16,192.168.32.0/22,{{ subnet.stdout }}.0/24"

   - name: Trim the GPU Driver Version
     shell: "echo {{ gpu_driver_version }} | awk -F'.' '{print $1}'"
     register: dversion

   - set_fact:
       driver_version: "{{ dversion.stdout }}"

   - name: Install GPU Operator with Confidential Computing
     when: "confidential_computing == true and enable_gpu_operator == true and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false"
     block:
       - name: Label Nodes with vm-passthrough for Confidential Computing
         shell: kubectl label nodes --all nvidia.com/gpu.workload.config=vm-passthrough

       - name: Install Confidential Containers
         shell: "export VERSION=v0.10.0; kubectl apply -k \"github.com/confidential-containers/operator/config/release?ref=${VERSION}\"; kubectl apply --dry-run=client -o yaml -k \"github.com/confidential-containers/operator/config/samples/ccruntime/default?ref=${VERSION}\" > {{ ansible_user_dir }}/ccruntime.yaml"

       - name: Replace node selector with nvidia.com for CC Runtime
         replace:
           path: "{{ ansible_user_dir }}/ccruntime.yaml"
           regexp: 'node.kubernetes.io/worker: ""'
           replace: 'nvidia.com/gpu.workload.config: "vm-passthrough"'
           backup: yes

       - name: Install CC Runtime
         shell: "kubectl apply -f {{ ansible_user_dir }}/ccruntime.yaml"

       - name: Install NVIDIA GPU Operator for Confidential Computing
         shell: helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --wait --generate-name -n nvidia-gpu-operator --create-namespace nvidia/gpu-operator --set sandboxWorkloads.enabled=true --set kataManager.enabled=true --set ccManager.enabled=true --set nfd.nodefeaturerules=true

       - name: Setting the Node Level Mode
         shell: kubectl label nodes --all nvidia.com/cc.mode=on --overwrite

   - name: Installing the GPU Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and cns_docker == true and cns_nvidia_driver == false and  enable_mig == false and enable_vgpu == false and enable_rdma == false and ngc_registry_password == ''"
     shell: helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator '{{ gpu_operator_helm_chart }}' --set toolkit.enabled=false --wait --generate-name

   - name: Installing the GPU Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and cns_docker == false and cns_nvidia_driver == true and enable_mig == false and enable_vgpu == false and enable_rdma == false and ngc_registry_password == ''"
     shell: helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator '{{ gpu_operator_helm_chart }}' --set driver.enabled=false --wait --generate-name

   - name: Installing the GPU Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and cns_docker == true and cns_nvidia_driver == true and enable_mig == false and enable_vgpu == false and enable_rdma == false and ngc_registry_password == ''"
     shell: helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator '{{ gpu_operator_helm_chart }}' --set driver.enabled=false,toolkit.enabled=false --wait --generate-name

   - name: Installing the GPU Operator with MIG on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and cns_docker == true and cns_nvidia_driver == true and enable_mig == true and enable_rdma == false  and enable_vgpu == false and ngc_registry_password == ''"
     shell: helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator '{{ gpu_operator_helm_chart }}' --set mig.strategy=single,driver.enabled=false,toolkit.enabled=false --wait --generate-name

   - name: Installing the GPU Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name"

   - name: Installing the GPU Operator with Open RM on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == true and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}',driver.useOpenKernelModules=true --wait --generate-name"

   - name: Create namespace and registry secret
     when: "confidential_computing == false and enable_gpu_operator == true and cns_nvidia_driver == false and ngc_registry_password != ''"
     shell: "{{ item }}"
     with_items:
       - kubectl create namespace nvidia-gpu-operator
       - kubectl create secret docker-registry ngc-secret --docker-server='https://nvcr.io' --docker-username='{{ ngc_registry_username }}' --docker-password='{{ ngc_registry_password }}' -n nvidia-gpu-operator

   - name: Installing the Signed GPU Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and cns_version < 7.2 and enable_gpu_operator == true and use_open_kernel_module == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == true and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.version='{{ driver_version }}'-signed,driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name"

   - name: Installing the Signed GPU Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and cns_version >= 7.2 and enable_gpu_operator == true and use_open_kernel_module == false and enable_mig == false and enable_vgpu == false and enable_rdma == false and enable_gds == false and enable_secure_boot == true and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "{{ item }}"
     with_items:
       - helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel {{ gpu_operator_helm_chart }} --set driver.version={{ driver_version }},driver.usePrecompiled=true,driver.repository={{ gpu_operator_driver_registry }} --wait --generate-name
       - sleep 20
       - kubectl patch clusterpolicy/cluster-policy --type='json' -p='[{"op":"replace", "path":"/spec/driver/usePrecompiled", "value":true},{"op":"replace", "path":"/spec/driver/version", "value":"{{ driver_version }}"}]'

   - name: Install GPU Operator with vGPU on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and enable_vgpu == true and cns_nvidia_driver == false and ngc_registry_password != ''"
     shell: "{{ item }}"
     with_items:
        - kubectl create namespace nvidia-gpu-operator
        - kubectl create configmap licensing-config -n nvidia-gpu-operator --from-file={{lookup('pipe', 'pwd')}}/files/gridd.conf --from-file={{lookup('pipe', 'pwd')}}/files/client_configuration_token.tok
        - kubectl create secret docker-registry registry-secret --docker-server='https://nvcr.io' --docker-username='{{ ngc_registry_username }}' --docker-password='{{ ngc_registry_password }}' --docker-email='{{ ngc_registry_email }}' -n nvidia-gpu-operator
        - helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.repository='{{ gpu_operator_driver_registry }}',driver.version='{{ gpu_driver_version }}',driver.imagePullSecrets[0]=registry-secret,driver.licensingConfig.configMapName=licensing-config --wait --generate-name

   - name: Installing the GPU Operator with MIG and Network Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and deploy_ofed == true and enable_mig == true and enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name"

   - name: Installing the GPU Operator with Network Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and deploy_ofed == true and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name"

   - name: Installing the GPU Operator with Network Operator and RDMA and GDS on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and deploy_ofed == true and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false  and ngc_registry_password == ''"
     shell: helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}',driver.imagePullSecrets[0]=ngc-secret,gds.enabled=true --wait --generate-name

   - name: Installing the Open RM GPU Operator with Network Operator and RDMA and GDS on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == true and deploy_ofed == true and enable_mig == false and enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false  and ngc_registry_password == ''"
     shell: helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}',driver.imagePullSecrets[0]=ngc-secret,gds.enabled=true,driver.useOpenKernelModules=true --wait --generate-name

   - name: Installing the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and deploy_ofed == false and enable_mig == true and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell:  helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name

   - name: Installing the Open RM GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == true and deploy_ofed == false and enable_mig == true and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell:  helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}',driver.useOpenKernelModules=true --wait --generate-name

   - name: Installing the GDS with Open RM GPU Operator on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == true and deploy_ofed == false and enable_mig == false and  enable_rdma == false and enable_vgpu == false and enable_gds == true and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell:  helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set gds.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}',driver.useOpenKernelModules=true --wait --generate-name

   - name: Installing the GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and deploy_ofed == false and enable_mig == false and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell:  helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name

   - name: Installing the Open RM GPU Operator with RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == true and deploy_ofed == false and enable_mig == false and  enable_rdma == true and enable_vgpu == false and enable_gds == false and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell:  helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}',driver.useOpenKernelModules=true --wait --generate-name

   - name: Installing the GPU Operator with GDS and RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and deploy_ofed == false and enable_mig == false and  enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,gds.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name"

   - name: Installing the Open RM GPU Operator with GDS and RDMA and Host MOFED on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == true and deploy_ofed == false and enable_mig == false and  enable_rdma == true and enable_vgpu == false and enable_gds == true and enable_secure_boot == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set driver.rdma.enabled=true,driver.rdma.useHostMofed=true,gds.enabled=true,driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}',driver.useOpenKernelModules=true --wait --generate-name"

   - name: Installing the GPU Operator with MIG on NVIDIA Cloud Native Stack
     when: "confidential_computing == false and enable_gpu_operator == true and use_open_kernel_module == false and enable_mig == true and  enable_rdma == false  and enable_vgpu == false and cns_nvidia_driver == false and ngc_registry_password == ''"
     shell: "helm install --version {{ gpu_operator_version }} --values {{ ansible_user_dir }}/values.yaml --create-namespace --namespace nvidia-gpu-operator --devel '{{ gpu_operator_helm_chart }}' --set mig.strategy='{{ mig_strategy }}',driver.version='{{ gpu_driver_version }}',driver.repository='{{ gpu_operator_driver_registry }}' --wait --generate-name"

   - name: GPU Operator Changes with CRI Docker Runtime
     shell: 'sleep 60; kubectl get clusterpolicy cluster-policy -o yaml | sed "/validator:/a\    driver:\n      env:\n      - name: DISABLE_DEV_CHAR_SYMLINK_CREATION\n        value: \"true\"" | kubectl apply -f -'
     when: "enable_gpu_operator == true and container_runtime == 'cri-dockerd'"

   - name: nodes length
     set_fact: nodes_group="{{ groups['nodes'] | length }}"

   - name: Container Networking Plugin changes
     when: "enable_gpu_operator == true and nodes_group | int >= 1"
     shell: "sleep 20; timeout 15 kubectl delete pods $(kubectl get pods -n kube-system | grep core | awk '{print $1}') -n kube-system; for ns in `kubectl get pods -A  | grep node-feature | grep -v master | awk '{print $1}'`; do kubectl get pods -n $ns  | grep node-feature | grep -v master | awk '{print $1}' | xargs kubectl delete pod -n $ns; done"

   - name: Install NIM Operator
     shell: "{{ item }}"
     when: "enable_nim_operator == true"
     with_items:
       - kubectl create namespace nim-operator
       - kubectl label --overwrite ns nim-operator pod-security.kubernetes.io/warn=privileged pod-security.kubernetes.io/enforce=privileged
       - helm install --version {{ nim_operator_version }} nim-operator nvidia/k8s-nim-operator -n nim-operator