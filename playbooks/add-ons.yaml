- hosts: nodes
  vars_files:
    - cns_values.yaml
  tasks:
   - name: Install NFS Packages on Ubuntu
     become: true
     when: storage == true and ansible_distribution == 'Ubuntu'
     apt:
      name: ['nfs-common']
      state: present
      update_cache: true
     retries: 5
     delay: 5
     register: install_nfs_common
     until: install_nfs_common is succeeded

- hosts: master
  vars_files:
    - cns_values.yaml
  tasks:
   - name: check Local Path Storage installed
     shell: kubectl get sc | awk '{print $1}' | grep local-path
     failed_when: false
     ignore_errors: true
     register: local_path

   - name: Install Local Path Provisoner on NVIDIA Cloud Native Stack
     shell: helm repo add containeroo https://charts.containeroo.ch; helm install local-path-provisioner containeroo/local-path-provisioner --version {{ local_path_provisioner }} --create-namespace --namespace local-path-storage --force
     when: storage == true and local_path.stdout == ''
     retries: 5
     delay: 5
     register: install_local_path_provisioner
     until: install_local_path_provisioner is succeeded

   - name: check NFS Storage installed
     shell: kubectl get sc | awk '{print $1}' | grep nfs
     failed_when: false
     ignore_errors: true
     register: nfs_storage

   - name: Install NFS Packages on RHEL
     become: true
     when: storage == true and ansible_distribution == 'RedHat' and nfs_storage.stdout == ''
     yum:
      name: ['nfs-utils']
      state: present
     retries: 5
     delay: 5
     register: install_nfs_utils
     until: install_nfs_utils is succeeded

   - name: Install NFS Packages on Ubuntu
     become: true
     when: storage == true and ansible_distribution == 'Ubuntu' and nfs_storage.stdout == ''
     apt:
      name: ['nfs-kernel-server', 'nfs-common']
      state: present
      update_cache: true
     retries: 5
     delay: 5
     register: install_nfs_packages
     until: install_nfs_packages is succeeded

   - name: Setup NFS provisioner
     become: true
     when: storage == true and nfs_storage.stdout == ''
     block:
       - name: Setup Mounts for NFS
         shell: |
           mkdir -p /data/nfs
           chown nobody:nogroup /data/nfs
           chmod 2770 /data/nfs
         retries: 5
         delay: 5
         register: setup_nfs_mounts
         until: setup_nfs_mounts is succeeded

       - name: Update Exports for NFS
         lineinfile:
           path: /etc/exports
           insertafter: EOF
           line: "/data/nfs  *(rw,sync,no_subtree_check,no_root_squash,insecure)"
         retries: 5
         delay: 5
         register: update_nfs_exports
         until: update_nfs_exports is succeeded

       - name: Run exports
         shell: exportfs -a
         retries: 5
         delay: 5
         register: run_exports
         until: run_exports is succeeded

   - name: NFS service restart service on Ubuntu
     become: true
     when: storage == true and ansible_distribution == 'Ubuntu' and nfs_storage.stdout == ''
     systemd_service:
       name: nfs-kernel-server
       state: restarted
       daemon_reload: yes
     retries: 5
     delay: 5
     register: restart_nfs_ubuntu
     until: restart_nfs_ubuntu is succeeded

   - name: NFS Service restart service on RHEL
     become: true
     when: storage == true and ansible_distribution == 'RedHat' and nfs_storage.stdout == ''
     systemd_service:
       name: nfs-server
       state: restarted
       daemon_reload: yes
     retries: 5
     delay: 5
     register: restart_nfs_rhel
     until: restart_nfs_rhel is succeeded

   - name: Install NFS External Provisioner
     when: storage == true and nfs_storage.stdout == ''
     shell: |
       helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner --force-update
       helm repo update
       helm install --version {{ nfs_provisioner }} nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server={{ ansible_default_ipv4.address }} --set nfs.path=/data/nfs --set storageClass.archiveOnDelete=false --create-namespace --namespace nfs-client --force
       sleep 10
       kubectl patch storageclass nfs-client -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
     retries: 5
     delay: 5
     register: install_nfs_provisioner
     until: install_nfs_provisioner is succeeded

   - name: check StorageClass is installed
     shell: kubectl get sc | tr -d '\n'
     failed_when: false
     ignore_errors: true
     register: storageclass

   - name: Copy files
     when: monitoring == true
     copy:
       src: "{{ item }}"
       dest: "{{ ansible_user_dir }}/"
     with_fileglob:
       - "{{lookup('pipe', 'pwd')}}/files/grafana.yaml"
       - "{{lookup('pipe', 'pwd')}}/files/kube-prometheus-stack.values"
       - "{{lookup('pipe', 'pwd')}}/files/fluent-values.yaml"
     retries: 5
     delay: 5
     register: copy_monitoring_files
     until: copy_monitoring_files is succeeded

   - name: check Metal LB installed
     shell: kubectl get pods -A | grep metallb | awk '{print $2}' | head -n 1 | wc -l | tr -d '\n'
     failed_when: false
     ignore_errors: true
     register: metallb

   - name: Install MetalLB
     shell: "helm repo add metallb https://metallb.github.io/metallb; helm install my-metallb metallb/metallb --version {{ metallb_version }} --create-namespace --namespace metallb-system --force; sleep 180"
     when: loadbalancer == true and metallb.stdout != '1'
     retries: 5
     delay: 5
     register: install_metallb
     until: install_metallb is succeeded

   - name: Get Host IP
     shell: ip route get 1 | awk '{print $7; exit}' | tr -d '\n'
     register: network
     retries: 5
     delay: 5
     until: network is succeeded

   - name: Local IP
     when: loadbalancer == true and metallb.stdout != '1'
     set_fact:
       load_balancer_ip: "{% if loadbalancer_ip == '' %}{{ network.stdout_lines[0] }}/32{%elif loadbalancer_ip != '' %}{{ loadbalancer_ip }}{% endif %}"

   - name: Apply Layer2 Config for MetalLB
     when: loadbalancer == true and metallb.stdout != '1'
     shell: |
       kubectl apply -f - <<EOF
       apiVersion: metallb.io/v1beta1
       kind: IPAddressPool
       metadata:
         name: first-pool
         namespace: metallb-system
       spec:
         addresses:
         - {{  load_balancer_ip }}
       ---
       apiVersion: metallb.io/v1beta1
       kind: L2Advertisement
       metadata:
         name: example
         namespace: metallb-system
       spec:
         ipAddressPools:
         - first-pool
       EOF
     retries: 5
     delay: 5
     register: apply_metallb_config
     until: apply_metallb_config is succeeded

   - name: check Elastic Stack Installed
     shell: kubectl get pods -A | grep elastic | awk '{print $2}' | head -n 1 | wc -l | tr -d '\n'
     failed_when: false
     ignore_errors: true
     register: elastic

   - name: Install Elastic Stack
     ignore_errors: true
     failed_when: false
     shell: "{{ item }}"
     when: monitoring == true and elastic.stdout != '1'
     with_items:
       - helm repo add elastic https://helm.elastic.co
       - helm repo add fluent https://fluent.github.io/helm-charts/
       - helm repo update
       - helm install elastic-operator elastic/eck-operator -n elastic-system --create-namespace --force
       - sleep 5
       - kubectl create ns monitoring
     retries: 5
     delay: 5
     register: install_elastic_stack
     until: install_elastic_stack is succeeded

   - name: Apply Elastic Config
     ignore_errors: true
     when:  monitoring == true and elastic.stdout != '1'
     shell: |
       kubectl apply -f - <<EOF
       apiVersion: elasticsearch.k8s.elastic.co/v1
       kind: Elasticsearch
       metadata:
         name: cloud-native
         namespace: monitoring
       spec:
         version: {{ elastic_stack }}
         nodeSets:
         - name: default
           count: 1
           volumeClaimTemplates:
           - metadata:
               name: elasticsearch-data # Do not change this name unless you set up a volume mount for the data path.
             spec:
               accessModes:
               - ReadWriteOnce
               resources:
                 requests:
                   storage: 5Gi
               storageClassName: nfs-client
           config:
            node.store.allow_mmap: false
       ---
       apiVersion: kibana.k8s.elastic.co/v1
       kind: Kibana
       metadata:
         name: cloud-native
         namespace: monitoring
       spec:
         version: {{ elastic_stack }}
         count: 1
         http:
          service:
            spec:
              type: NodePort
         elasticsearchRef:
           name: cloud-native
       EOF
     retries: 5
     delay: 5
     register: apply_elastic_config
     until: apply_elastic_config is succeeded

   - name: Install Fluent Bit
     shell: "{{ item }}"
     when: monitoring == true and elastic.stdout != '1'
     ignore_errors: true
     failed_when: false
     with_items:
       - sleep 10
       - "kubectl get secrets -n monitoring cloud-native-es-elastic-user -o yaml | sed \"s/  elastic: .*/  elastic: Y25zLXN0YWNr/g\"  | kubectl apply -f -"
       - kubectl delete pod $(kubectl get pod -n monitoring | grep cloud-native | awk '{print $1}') -n monitoring --force
       - "helm install -f {{ ansible_user_dir }}/fluent-values.yaml fluent/fluent-bit -n monitoring --generate-name"
       - "curl -u 'elastic:cns-stack' -X POST -k \"https://$(kubectl get svc -n monitoring | grep cloud-native-kb-http | awk '{print $3}'):5601/api/data_views/data_view\" -H 'kbn-xsrf: true' -H 'Content-Type: application/json' -d' {  \"data_view\": { \"title\": \"logs*\", \"name\": \"My Logs\"}}'"
       - kubectl patch svc $(kubectl get svc -n monitoring | grep cloud-native-kb-http | awk '{print $1}') --type='json' -p '[{"op":"replace","path":"/spec/type","value":"NodePort"},{"op":"replace","path":"/spec/ports/0/nodePort","value":32221}]' -n monitoring
     retries: 5
     delay: 5
     register: install_fluent_bit
     until: install_fluent_bit is succeeded

   - name: Check Prometheus is installed
     shell: kubectl get pods -A | grep prometheus | awk '{print $2}' | head -n 1 | wc -l | tr -d '\n'
     failed_when: false
     ignore_errors: true
     register: prometheus

   - name: Install Prometheus Stack on NVIDIA Cloud Native Stack
     ignore_errors: true
     when: monitoring == true and prometheus.stdout != '1'
     block:
       - name: Add Prometheus Helm repository
         shell: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
         register: helm_repo_add
         retries: 5
         delay: 5
         until: helm_repo_add is succeeded

       - name: Update Helm repositories
         shell: helm repo update
         when: helm_repo_add.rc == 0
         retries: 5
         delay: 5
         register: update_helm_repos
         until: update_helm_repos is succeeded

       - name: Install Prometheus
         shell: "helm install kube-prometheus-stack --version {{ prometheus_stack }} prometheus-community/kube-prometheus-stack --create-namespace --namespace monitoring --values  {{ ansible_user_dir }}/kube-prometheus-stack.values"
         when: helm_repo_add.rc == 0
         retries: 5
         delay: 5
         register: install_prometheus
         until: install_prometheus is succeeded

       - name: Install Grafana Operator
         shell: "helm upgrade -i grafana-operator oci://ghcr.io/grafana/helm-charts/grafana-operator --version v{{ grafana_operator }} -n monitoring"
         retries: 5
         delay: 5
         register: install_grafana_operator
         until: install_grafana_operator is succeeded

       - name: Apply Grafana configuration
         shell: kubectl apply -f {{ ansible_user_dir }}/grafana.yaml -n monitoring
         retries: 5
         delay: 5
         register: apply_grafana_config
         until: apply_grafana_config is succeeded

       - name: Install Prometheus Adapter
         shell: "helm install --version {{ prometheus_adapter }} prometheus-adapter prometheus-community/prometheus-adapter --namespace monitoring --set prometheus.url=http://kube-prometheus-stack-prometheus.monitoring,prometheus.port=9090"
         when: helm_repo_add.rc == 0
         retries: 5
         delay: 5
         register: install_prometheus_adapter
         until: install_prometheus_adapter is succeeded

       - name: Apply Metrics Server configuration
         shell: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
         retries: 5
         delay: 5
         register: apply_metrics_server
         until: apply_metrics_server is succeeded

       - name: Patch Metrics Server Deployment
         shell: "kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--kubelet-insecure-tls\"}]'"
         retries: 5
         delay: 5
         register: patch_metrics_server
         until: patch_metrics_server is succeeded

       - name: Patch Cluster Policy for DCGM Exporter
         shell: "kubectl patch clusterpolicy/cluster-policy --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/dcgmExporter/serviceMonitor/enabled\", \"value\": true}]'"
         retries: 5
         delay: 5
         register: patch_cluster_policy
         until: patch_cluster_policy is succeeded

       - name: Delete DCGM Pod
         shell: kubectl delete pod $(kubectl get pods -n nvidia-gpu-operator | grep dcgm | awk '{print $1}') -n nvidia-gpu-operator --force
         retries: 5
         delay: 5
         register: delete_dcgm_pod
         until: delete_dcgm_pod is succeeded

   - name: check LeaderWorkerSet is installed
     shell: kubectl get pods -A | grep lws | awk '{print $2}' | head -n 1 | wc -l | tr -d '\n'
     failed_when: false
     ignore_errors: true
     register: lw_set

   - name: Install LeaderWorkerSet on NVIDIA Cloud Native Stack
     ignore_errors: true
     failed_when: false
     shell: helm install lws https://github.com/kubernetes-sigs/lws/releases/download/v{{ lws_version }}/lws-chart-{{ lws_version }}.tgz --namespace lws-system  --create-namespace  --wait --timeout 300s
     when: lws == true and lw_set.stdout != '1'
     retries: 5
     delay: 5
     register: install_lws
     until: install_lws is succeeded

   - name: check Volcano Scheduler is installed
     shell: kubectl get pods -A | grep volcano | awk '{print $2}' | head -n 1 | wc -l | tr -d '\n'
     failed_when: false
     ignore_errors: true
     register: volcano_scheduler

   - name: Install Volcano Scheduler
     when: volcano == true and volcano_scheduler.stdout != '1'
     shell: |
       helm repo add volcano-sh https://volcano-sh.github.io/helm-charts
       helm repo update
       helm install volcano volcano-sh/volcano --version {{ volcano_version }} --namespace volcano-system --create-namespace --force
     retries: 5
     delay: 5
     register: install_volcano_scheduler
     until: install_volcano_scheduler is succeeded

   - name: check Kserve is Installed
     shell: kubectl get pods -A | grep kserve | awk '{print $2}' | head -n 1 | wc -l | tr -d '\n'
     failed_when: false
     ignore_errors: true
     register: kserve_install  

   - name: Install Kserve on Kubernetes
     when: kserve == true and kserve_install.stdout != '1'
     retries: 5
     delay: 30
     until: install_kserve is succeeded
     shell: |
       kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/${GATEWAY_API_VERSION}/standard-install.yaml
       helm repo add istio https://istio-release.storage.googleapis.com/charts --force-update
       helm install istio-base istio/base -n istio-system --wait --set defaultRevision=default --create-namespace --version ${ISTIO_VERSION}
       helm install istiod istio/istiod -n istio-system --wait --version ${ISTIO_VERSION} --set proxy.autoInject=disabled --set-string pilot.podAnnotations."cluster-autoscaler\.kubernetes\.io/safe-to-evict"=true
       helm install istio-ingressgateway istio/gateway -n istio-system --version ${ISTIO_VERSION} --set-string podAnnotations."cluster-autoscaler\.kubernetes\.io/safe-to-evict"=true
       sleep 20
       helm repo add jetstack https://charts.jetstack.io --force-update
       helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace  --version ${CERT_MANAGER_VERSION}  --set crds.enabled=true
       helm install knative-operator --namespace knative-serving --create-namespace --wait https://github.com/knative/operator/releases/download/knative-${KNATIVE_OPERATOR_VERSION}/knative-operator-${KNATIVE_OPERATOR_VERSION}.tgz
       kubectl apply -f - <<EOF
       apiVersion: operator.knative.dev/v1beta1
       kind: KnativeServing
       metadata:
         name: knative-serving
         namespace: knative-serving
       spec:
         version: "${KNATIVE_SERVING_VERSION}"
         config:
           domain:
         # Patch the external domain as the default domain svc.cluster.local is not exposed on ingress (from knative 1.8)
             example.com: ""
       EOF
       helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd --version ${KSERVE_VERSION} --namespace kserve --create-namespace --wait
       helm install kserve oci://ghcr.io/kserve/charts/kserve --version ${KSERVE_VERSION} --namespace kserve --create-namespace --wait --set-string kserve.controller.deploymentMode="Serverless"
     register: install_kserve
     failed_when: false
     ignore_errors: true
     environment:
       ISTIO_VERSION: "1.28.0"
       KNATIVE_OPERATOR_VERSION: "v1.20.0"
       KNATIVE_SERVING_VERSION: "1.20.0"
       KSERVE_VERSION: "v0.16.0"
       CERT_MANAGER_VERSION: "v1.19.1"
       GATEWAY_API_VERSION: "v1.4.0"

   - name: Patch Knative Serving CRD
     when: kserve == true
     shell: |
       sleep 15; kubectl patch configmap config-features -n knative-serving --type merge -p '{"data":{"kubernetes.podspec-nodeselector":"enabled"}}'
       kubectl patch configmap config-features -n knative-serving --type merge -p '{"data":{"kubernetes.podspec-persistent-volume-claim":"enabled","kubernetes.podspec-persistent-volume-write":"enabled"}}'
     retries: 5
     delay: 5
     ignore_errors: true
     failed_when: false
     register: patch_knative_serving_crd
     until: patch_knative_serving_crd is succeeded

   - name: Apply Nginx IngressClass Name for Kserve
     when: kserve == true and kserve_install.stdout != '1'
     shell: |
       kubectl apply -f - <<EOF
       apiVersion: networking.k8s.io/v1
       kind: IngressClass
       metadata:
         name: nginx
       spec:
         controller: istio.io/ingress-controller
       EOF
     retries: 5
     delay: 5
     register: apply_ingress_config
     until: apply_ingress_config is succeeded